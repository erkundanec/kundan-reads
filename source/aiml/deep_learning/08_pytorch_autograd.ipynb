{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorchâ€™s Autograd module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring PyTorch's `autograd` module allows us to understand how PyTorch enables automatic differentiation for computing gradients during neural network training. Let's dive into the `autograd` module and explore its functionalities:\n",
    "\n",
    "### 1. Enabling Automatic Differentiation\n",
    "\n",
    "PyTorch's `autograd` module automatically tracks operations performed on tensors and computes gradients with respect to those tensors. We can enable automatic differentiation by setting the `requires_grad` attribute of tensors to `True`.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Enable automatic differentiation for a tensor\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "```\n",
    "\n",
    "### 2. Tracking Computational History\n",
    "\n",
    "PyTorch keeps track of the operations performed on tensors to compute gradients efficiently. Each tensor has a `.grad_fn` attribute that references the operation that created it.\n",
    "\n",
    "```python\n",
    "# Perform operations on the tensor\n",
    "y = x**2 + 2*x + 1\n",
    "\n",
    "# Print the computational history\n",
    "print(y.grad_fn)\n",
    "```\n",
    "\n",
    "### 3. Computing Gradients\n",
    "\n",
    "After performing operations on tensors, we can compute gradients of a scalar-valued tensor with respect to other tensors using the `.backward()` method.\n",
    "\n",
    "```python\n",
    "# Define a scalar-valued tensor\n",
    "loss = y**2\n",
    "\n",
    "# Perform backpropagation to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Access gradients\n",
    "print(x.grad)\n",
    "```\n",
    "\n",
    "### 4. Disabling Gradient Tracking\n",
    "\n",
    "Sometimes, we may want to perform computations without tracking gradients. We can temporarily disable gradient tracking using the `torch.no_grad()` context manager.\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    # Perform operations without tracking gradients\n",
    "    ...\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "PyTorch's `autograd` module provides powerful automatic differentiation capabilities, enabling efficient computation of gradients during neural network training. By exploring the functionalities of the `autograd` module, we gain a deeper understanding of how PyTorch handles gradient computation behind the scenes, making it easier to develop and debug machine learning models effectively. Experimenting with `autograd` functionalities is essential for mastering PyTorch and building advanced neural network architectures."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
