{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing backpropagation manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing backpropagation manually in PyTorch involves computing gradients of a loss function with respect to model parameters using the chain rule. Let's go through the steps to perform backpropagation manually:\n",
    "\n",
    "### 1. Define the Model\n",
    "\n",
    "First, we define a simple neural network model. Here's an example:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "```\n",
    "\n",
    "### 2. Define the Loss Function\n",
    "\n",
    "Next, we define a loss function. We'll use Mean Squared Error (MSE) loss in this example:\n",
    "\n",
    "```python\n",
    "criterion = nn.MSELoss()\n",
    "```\n",
    "\n",
    "### 3. Perform Forward Pass\n",
    "\n",
    "Now, we perform a forward pass through the model to compute predictions and the corresponding loss:\n",
    "\n",
    "```python\n",
    "# Example input and target\n",
    "input_data = torch.randn(1, 10)\n",
    "target = torch.randn(1, 1)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data)\n",
    "loss = criterion(output, target)\n",
    "```\n",
    "\n",
    "### 4. Perform Backward Pass Manually\n",
    "\n",
    "After computing the loss, we manually perform the backward pass to compute gradients of the loss with respect to model parameters:\n",
    "\n",
    "```python\n",
    "# Zero the gradients\n",
    "model.zero_grad()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Access gradients\n",
    "for param in model.parameters():\n",
    "    print(param.grad)\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Performing backpropagation manually in PyTorch involves computing gradients of the loss function with respect to model parameters using the chain rule. By understanding the steps involved in manual backpropagation, we gain insight into how PyTorch handles gradient computation automatically during training. Manual backpropagation can be useful for debugging and understanding the inner workings of neural network training algorithms."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
