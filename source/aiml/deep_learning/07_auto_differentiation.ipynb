{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Automatic Differentiation\n",
    "\n",
    "Automatic Differentiation (AD) is a computational technique used in mathematical optimization and machine learning for efficiently computing derivatives of functions. It plays a crucial role in training neural networks and other optimization tasks where gradients are essential for updating model parameters. In this introduction, we'll cover the basic concepts of automatic differentiation and its significance in machine learning.\n",
    "\n",
    "### 1. What is Automatic Differentiation?\n",
    "\n",
    "Automatic Differentiation is a method for computing derivatives of functions by decomposing them into a sequence of elementary arithmetic operations and elementary functions. Unlike symbolic differentiation (e.g., using algebraic rules to differentiate expressions), automatic differentiation operates directly on the numerical values of functions, making it more efficient and suitable for numerical optimization tasks.\n",
    "\n",
    "### 2. How Does Automatic Differentiation Work?\n",
    "\n",
    "Automatic Differentiation works by decomposing a complex function into a sequence of elementary operations, each of which has a known derivative. It then applies the chain rule recursively to compute the derivatives of the entire function. This process is typically done in two modes:\n",
    "\n",
    "- **Forward Mode**: Evaluates the function and its derivative simultaneously in a forward pass.\n",
    "- **Reverse Mode**: Computes the derivative of the function with respect to each input variable in a backward pass.\n",
    "\n",
    "Let's illustrate the concepts of Automatic Differentiation, Forward Mode, and Reverse Mode with a simple mathematical function and its computational graph.\n",
    "\n",
    "Consider the following function:\n",
    "\n",
    "$$ f(x, y) = (x + y) \\times (x - y)$$\n",
    "\n",
    "#### Forward Mode:\n",
    "\n",
    "In the forward mode, we evaluate the function and its derivative simultaneously in a forward pass. Here's how we compute $ f(x, y) $ and its derivative $\\frac{{df}}{{dx}}$ and $\\frac{{df}}{{dy}}$ using the forward mode:\n",
    "\n",
    "1. **Forward Pass**: We evaluate the function $f(x, y)$ by performing the elementary operations in a forward direction:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a &= x + y \\\\\n",
    "b &= x - y \\\\\n",
    "f &= a \\times b = (x + y) \\times (x - y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "2. **Derivative Evaluation**: We simultaneously evaluate the derivatives of the function with respect to each input variable:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{{df}}{{dx}} &= \\frac{{df}}{{da}} \\cdot \\frac{{da}}{{dx}} + \\frac{{df}}{{db}} \\cdot \\frac{{db}}{{dx}} \\\\\n",
    "\\frac{{df}}{{dy}} &= \\frac{{df}}{{da}} \\cdot \\frac{{da}}{{dy}} + \\frac{{df}}{{db}} \\cdot \\frac{{db}}{{dy}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Reverse Mode:\n",
    "\n",
    "In the reverse mode, we compute the derivative of the function with respect to each input variable in a backward pass. Here's how we compute $\\frac{{df}}{{dx}}$ and $\\frac{{df}}{{dy}}$ using the reverse mode:\n",
    "\n",
    "1. **Backward Pass**: We start from the output of the function and apply the chain rule recursively to compute the derivatives of the function with respect to each input variable:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{{df}}{{da}} &= \\frac{{df}}{{da}} \\cdot \\frac{{da}}{{dx}} + \\frac{{df}}{{da}} \\cdot \\frac{{da}}{{dy}} \\\\\n",
    "\\frac{{df}}{{db}} &= \\frac{{df}}{{db}} \\cdot \\frac{{db}}{{dx}} + \\frac{{df}}{{db}} \\cdot \\frac{{db}}{{dy}} \\\\\n",
    "\\frac{{df}}{{dx}} &= \\frac{{df}}{{da}} \\cdot \\frac{{da}}{{dx}} + \\frac{{df}}{{db}} \\cdot \\frac{{db}}{{dx}} \\\\\n",
    "\\frac{{df}}{{dy}} &= \\frac{{df}}{{da}} \\cdot \\frac{{da}}{{dy}} + \\frac{{df}}{{db}} \\cdot \\frac{{db}}{{dy}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Computational Graph:\n",
    "\n",
    "Let's visualize the computational graph for the function $$f(x, y) = (x + y) \\times (x - y)$$:\n",
    "\n",
    "```\n",
    "    x -----\\\n",
    "            +---- a ----\\\n",
    "    y -----/            \\\n",
    "                         *\n",
    "    x -----\\            /\n",
    "            ----- b ----/\n",
    "    y -----/\n",
    "```\n",
    "\n",
    "In the graph:\n",
    "- Nodes represent operations (addition, subtraction, multiplication).\n",
    "- Edges represent the flow of data (variables $x$ and $y$).\n",
    "- We can traverse the graph both forwards (to compute $f(x, y)$) and backwards (to compute $\\frac{{df}}{{dx}}$ and $\\frac{{df}}{{dy}} $).\n",
    "\n",
    "\n",
    "Automatic Differentiation, Forward Mode, and Reverse Mode are powerful techniques for computing derivatives efficiently. They enable us to compute gradients of complex functions with respect to multiple input variables, which is essential for training neural networks and optimizing machine learning models. Understanding these concepts and visualizing computational graphs can deepen our understanding of how gradients are computed and used in machine learning algorithms.\n",
    "\n",
    "### 3. Significance in Machine Learning\n",
    "\n",
    "In machine learning, Automatic Differentiation is crucial for training neural networks using gradient-based optimization algorithms like Stochastic Gradient Descent (SGD). By efficiently computing gradients of the loss function with respect to model parameters, AD enables the optimization algorithm to update the parameters in the direction that minimizes the loss, thus improving the model's performance over time.\n",
    "\n",
    "### 4. Implementation in Libraries like PyTorch\n",
    "\n",
    "Libraries like PyTorch provide built-in support for automatic differentiation through modules like `autograd`. These libraries allow users to define computational graphs with tensors and automatically compute gradients of the graph's output with respect to its inputs. This makes it easy to train complex neural network models with minimal manual effort, as the library handles the gradient computations efficiently.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Automatic Differentiation is a powerful tool in the field of mathematical optimization and machine learning, enabling efficient computation of derivatives for functions with complex structures. By automating the process of gradient computation, AD simplifies the training of neural networks and other optimization tasks, making it easier to develop and deploy machine learning models effectively. Understanding the basics of automatic differentiation is essential for anyone working in the field of machine learning and optimization."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
