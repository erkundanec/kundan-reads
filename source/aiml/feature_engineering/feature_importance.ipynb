{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "## Using Regression Methods\n",
    "\n",
    "Different regression methods can estimate feature importance by providing coefficients or weights for the input features. Here’s a list of popular regression techniques that yield feature importance:\n",
    "\n",
    "1. **Linear Regression**\n",
    "2. **Ridge Regression**\n",
    "3. **Lasso Regression**\n",
    "4. **Elastic Net Regression**\n",
    "5. **Decision Tree Regression**\n",
    "6. **Random Forest Regression**\n",
    "7. **Gradient Boosting Regression (e.g., XGBoost)**\n",
    "8. **Support Vector Regression (SVR)**\n",
    "9. **Partial Least Squares (PLS) Regression**\n",
    "10. **Permutation Importance (Model Agnostic)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Generating the Dataset\n",
    "We'll create a simple dataset with `scikit-learn`'s `make_regression` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (100, 5)\n",
      "Target shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "\n",
    "# Print shape for verification\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression \n",
    "This is an Ordinary Least Squares.\n",
    "\n",
    "   - **How it works:** Minimizes the sum of squared errors to fit a linear model.\n",
    "   - **Feature Importance:** Coefficients of the linear model indicate the influence of each feature.\n",
    "   - **Interpretation:** Larger absolute values of coefficients indicate more importance.\n",
    "   \n",
    "   > **Limitation:** Sensitive to multicollinearity.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.linear_model import LinearRegression\n",
    "\n",
    "   # Initialize and fit the model\n",
    "   linear_model = LinearRegression()\n",
    "   linear_model.fit(X, y)\n",
    "\n",
    "   # Display feature importance (coefficients)\n",
    "   print(\"Linear Regression Coefficients:\", linear_model.coef_)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Ridge Regression (L2 Regularization)**\n",
    "   - **How it works:** Adds an L2 penalty to the loss function to shrink coefficients.\n",
    "   - **Feature Importance:** Similar to linear regression, but smaller coefficients due to the penalty.\n",
    "   - **Use case:** When features are correlated.\n",
    "   \n",
    "   > **Advantage:** Helps with multicollinearity and overfitting.\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Initialize Ridge Regression\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X, y)\n",
    "\n",
    "# Display feature importance (coefficients)\n",
    "print(\"Ridge Regression Coefficients:\", ridge_model.coef_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Lasso Regression (L1 Regularization)**\n",
    "   - **How it works:** Adds an L1 penalty that encourages sparsity in the coefficients.\n",
    "   - **Feature Importance:** Some coefficients are exactly zero, making it a feature selector.\n",
    "   - **Use case:** When you want to identify a small subset of important features.\n",
    "   \n",
    "   > **Advantage:** Automatic feature selection by shrinking unimportant coefficients to zero.\n",
    "\n",
    "   ```python\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Initialize Lasso Regression\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X, y)\n",
    "\n",
    "# Display feature importance (coefficients)\n",
    "print(\"Lasso Regression Coefficients:\", lasso_model.coef_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Elastic Net Regression (L1 + L2 Regularization)**\n",
    "   - **How it works:** Combines both L1 and L2 regularization.\n",
    "   - **Feature Importance:** Balances between feature selection (L1) and coefficient shrinkage (L2).\n",
    "   - **Use case:** When features are highly correlated, and some sparsity is desired.\n",
    "   \n",
    "   > **Advantage:** More flexible than Ridge or Lasso alone.\n",
    "\n",
    "   ```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Initialize Elastic Net Regression\n",
    "elastic_net_model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net_model.fit(X, y)\n",
    "\n",
    "# Display feature importance (coefficients)\n",
    "print(\"Elastic Net Coefficients:\", elastic_net_model.coef_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **Decision Tree Regression**\n",
    "   - **How it works:** Splits data at nodes based on feature values to minimize variance.\n",
    "   - **Feature Importance:** Based on reduction in variance or impurity at each split.\n",
    "   - **Interpretation:** Sum of reductions at each node where the feature was used.\n",
    "   \n",
    "   > **Limitation:** Prone to overfitting on small datasets.\n",
    "\n",
    "   ```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize Decision Tree Regressor\n",
    "tree_model = DecisionTreeRegressor()\n",
    "tree_model.fit(X, y)\n",
    "\n",
    "# Display feature importance\n",
    "print(\"Decision Tree Feature Importances:\", tree_model.feature_importances_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. **Random Forest Regression**\n",
    "   - **How it works:** Builds an ensemble of decision trees.\n",
    "   - **Feature Importance:** Average reduction in impurity across all trees where the feature was used.\n",
    "   - **Use case:** When non-linear relationships are important.\n",
    "   \n",
    "   > **Advantage:** More robust than a single decision tree.\n",
    "\n",
    "   ```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize Random Forest Regressor\n",
    "forest_model = RandomForestRegressor(n_estimators=100)\n",
    "forest_model.fit(X, y)\n",
    "\n",
    "# Display feature importance\n",
    "print(\"Random Forest Feature Importances:\", forest_model.feature_importances_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. **Gradient Boosting Regression (e.g., XGBoost, LightGBM)**\n",
    "   - **How it works:** Sequentially builds trees, each correcting the errors of the previous.\n",
    "   - **Feature Importance:** Based on gain (reduction in loss) or frequency of usage in trees.\n",
    "   - **Use case:** Excellent for complex, non-linear relationships.\n",
    "   \n",
    "   > **Advantage:** Often more accurate, with detailed feature importance scores.\n",
    "\n",
    "   ```python\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Initialize XGBoost Regressor\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "# Display feature importance\n",
    "print(\"XGBoost Feature Importances:\", xgb_model.feature_importances_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. **Support Vector Regression (SVR)**\n",
    "   - **How it works:** Finds a hyperplane in a high-dimensional space to minimize the error.\n",
    "   - **Feature Importance:** Based on the coefficients in the dual representation.\n",
    "   \n",
    "   > **Limitation:** Not straightforward for feature importance unless using linear kernels.\n",
    "\n",
    "   ```python\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Initialize Support Vector Regressor\n",
    "svr_model = SVR(kernel='linear')\n",
    "svr_model.fit(X, y)\n",
    "\n",
    "# Display feature importance (coefficients)\n",
    "print(\"SVR Coefficients:\", svr_model.coef_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. **Partial Least Squares (PLS) Regression**\n",
    "   - **How it works:** Projects predictors to a new space while maximizing variance explanation.\n",
    "   - **Feature Importance:** Feature weights on the projected latent variables.\n",
    "   \n",
    "   > **Use case:** Suitable for multicollinear and high-dimensional data.\n",
    "\n",
    "   ```python\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# Initialize PLS Regression with 2 components\n",
    "pls_model = PLSRegression(n_components=2)\n",
    "pls_model.fit(X, y)\n",
    "\n",
    "# Display the coefficients for each feature\n",
    "print(\"PLS Regression Coefficients:\", pls_model.coef_)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. **Permutation Importance (Model Agnostic)**\n",
    "   - **How it works:** Measures the decrease in model performance when a feature's values are randomly shuffled.\n",
    "   - **Feature Importance:** Drop in performance (e.g., R² or RMSE) after shuffling.\n",
    "   \n",
    "   > **Advantage:** Applicable to any model, providing a global view of feature importance.\n",
    "\n",
    "   ```python\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Use the previously trained RandomForest model\n",
    "perm_importance = permutation_importance(forest_model, X, y, n_repeats=30, random_state=42)\n",
    "\n",
    "# Display permutation importance\n",
    "print(\"Permutation Importances:\", perm_importance.importances_mean)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Method\n",
    "- **Linear Data:** Linear, Ridge, Lasso.\n",
    "- **Non-linear Data:** Random Forest, Gradient Boosting.\n",
    "- **Feature Selection Focus:** Lasso, Elastic Net.\n",
    "- **Interpretability:** Linear models, Decision Trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
