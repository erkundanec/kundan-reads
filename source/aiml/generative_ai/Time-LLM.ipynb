{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-LLM: Time to forecast Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not the first time that researchers try to apply natural language processing (NLP) techniques to the field of time series.\n",
    "\n",
    "For example, the Transformer architecture was a significant milestone in NLP, but its performance in time series forecasting remained average, until PatchTST was proposed.\n",
    "\n",
    "As you know, large language models (LLMs) are being actively developed and have demonstrated impressive generalization and reasoning capabilities in NLP. Thus, it is worth exploring the idea of repurposing an LLM for time series forecasting, such that we can benefit from the capabilities of those large pre-trained models.\n",
    "\n",
    "To that end, Time-LLM was proposed. In the original paper, the researchers propose a framework to reprogram an existing LLM to perform time series forecasting.\n",
    "\n",
    "In this article, we explore the architecture of Time-LLM and how it can effectively allow an LLM to predict time series data. Then, we implement the model and apply it in a small forecasting project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Time-LLM\n",
    "Time-LLM is to be considered more as a framework than an actual model with a specific architecture.\n",
    "\n",
    "The general structure of Time-LLM is shown below.\n",
    "\n",
    "<figure style=\"width: 800px;\">\n",
    "  <img src=\"img/time-LLM.png\" alt=\"Time-LLM\" width=\"800\">\n",
    "  <figcaption style=\"font-size: 12px;\">Fig General structure of Time-LLM. Image by M. Jin, S. Wang, L. Ma, Z. Chu, J. Zhang, X. Shi, P. Chen, Y. Liang, Y. Li, S. Pan, Q. Wen from Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.</figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire idea behind Time-LLM is to reprogram an embedding-visible language foundation model, like LLaMA or GPT-2. Note that this is different from fine-tuning the LLM. Instead, we teach the LLM to take an input sequence of time steps and output forecasts over a certain horizon. This means that the LLM itself stays unchanged.\n",
    "\n",
    "At a high level, Time-LLM starts by tokenizing the input time series sequence with a customized patch embedding layer. These patches are then sent through a reprogramming layer that essentially translate the forecasting task into a language task. Note that we can also pass a prompt prefix to augment the model's reasoning ability. Finally, the output patches go through the projection layer to ultimately get forecasts.\n",
    "\n",
    "There is a lot to dissect here, so let's explore each step in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input patching\n",
    "The first step is to patch the input series, just like in PatchTST.\n",
    "\n",
    "<figure style=\"width: 600px;\">\n",
    "  <img src=\"img/time-LLM_01.png\" alt=\"Time-LLM\" width=\"600\">\n",
    "  <figcaption style=\"font-size: 12px;\">Fig: Visualizing patching. Here, we have a sequence of 15 timesteps, with a patch length of 5 and a stride of 5 as well, resulting in three patches. Image by the author.</figcaption>\n",
    "</figure>\n",
    "\n",
    "With patching, the goal is to preserve the local semantic information by looking at groups of time steps instead of looking at a single time step.\n",
    "\n",
    "It also has the added benefit of greatly reducing the number of token being fed to the reprogramming layer. Here, each patch becomes an input token so it reduces the number of tokens from $L$ to approximately $L/S$, where $L$ is the length of the input sequence, and $S$ is the stride length.\n",
    "\n",
    "Once patching is done, the input sequence is sent to the reprogramming layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprogramming layer\n",
    "In Time-LLM, the language model stays intact.\n",
    "\n",
    "Now, a language model can perform many NLP tasks, like sentiment analysis, summarization and text generation, but time series forecasting.\n",
    "\n",
    "This is where the reprogramming layer comes in. It essentially maps the input time series into a language task, allowing us to leverage the capabilities of the language model.\n",
    "\n",
    "To do so, it uses a restricted vocabulary to describe each input patch, as shown below.\n",
    "\n",
    "<figure style=\"width: 600px;\">\n",
    "  <img src=\"img/time-LLM_02.png\" alt=\"Time-LLM\" width=\"600\">\n",
    "  <figcaption style=\"font-size: 12px;\">Fig: Transforming the input patch into a language task. Image by M. Jin, S. Wang, L. Ma, Z. Chu, J. Zhang, X. Shi, P. Chen, Y. Liang, Y. Li, S. Pan, Q. Wen from Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we can see how each time series patch gets described. For example, one patch could be translated to \"short up then steadily down\". That way, we effectively encode the behavior of the time series as a natural language input, which is what the LLM expects.\n",
    "\n",
    "Once this is done, the translated patches are sent to a multi-head attention mechanism and a linear projection is done to align the dimension of the reprogrammed patches to the dimension of the LLM backbone.\n",
    "\n",
    "<figure style=\"width: 600px; text-align: center;\">\n",
    "  <img src=\"img/time-LLM_03.png\" alt=\"Time-LLM\" width=\"400\">\n",
    "  <figcaption style=\"font-size: 12px;\">Fig: Overall architecture of the reprogramming layer. Image by M. Jin, S. Wang, L. Ma, Z. Chu, J. Zhang, X. Shi, P. Chen, Y. Liang, Y. Li, S. Pan, Q. Wen from Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the reprogramming layer is a trained layer. We can decide to train it for a specific dataset, or pre-train it and use Time-LLM as a zero-shot forecaster.\n",
    "\n",
    "Now, before the translated patches are actually sent to the LLM, it is possible to augment the input using a prompt prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment the input with Prompt-as-Prefix\n",
    "To activate the LLM's capabilities, we use a prompt, which a natural language input specifying the task for the LLM.\n",
    "\n",
    "Now, even though we are passing patches of time series translated to natural language, it still represents a challenge for the LLM to make predictions.\n",
    "\n",
    "Therefore, the researchers propose to use a prompt prefix to complement patch reprogramming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"width: 600px; text-align: center;\">\n",
    "  <img src=\"img/time-LLM_04.png\" alt=\"Time-LLM\" width=\"400\">\n",
    "  <figcaption style=\"font-size: 12px;\">Fig: Example of a prompt prefix. Image by M. Jin, S. Wang, L. Ma, Z. Chu, J. Zhang, X. Shi, P. Chen, Y. Liang, Y. Li, S. Pan, Q. Wen from Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we see an example of prompt prefix on the benchmark dataset ETT.\n",
    "\n",
    "The prompt contains three distinct parts:\n",
    "\n",
    "- General context of the dataset\n",
    "- Task instructions\n",
    "- Input statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component is entirely defined by the user. This is where we can specify information on the dataset, explain its context and write out the observations.\n",
    "\n",
    "Then, the task instructions are set programmatically given the horizon of the forecast and the input size of the series.\n",
    "\n",
    "Finally, the input statistics are also computed automatically given the input series. Note that the top five lags are calculated using a fast Fourier transform. In short, it transforms the input series into a function of amplitude and frequency, and the frequencies with the highest amplitudes are considered to be more important.\n",
    "\n",
    "So, at this point, we have a prompt prefix containing context and instructions for the LLM, as well as reprogrammed patches of the series being fed to the LLM, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"width: 600px; text-align: center;\">\n",
    "  <img src=\"img/time-LLM_05.png\" alt=\"Time-LLM\" width=\"400\">\n",
    "  <figcaption style=\"font-size: 12px;\">Fig: The prompt prefix and repgroammed patches are sent to the LLM. The final step is a linear projection to get the final predictions. Image by M. Jin, S. Wang, L. Ma, Z. Chu, J. Zhang, X. Shi, P. Chen, Y. Liang, Y. Li, S. Pan, Q. Wen from Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in the framework is to send the output patch embeddings through a linear projection layer to get the final predictions.\n",
    "\n",
    "Output projection\n",
    "Once the prompt prefix and reprogrammed patches are sent to the LLM, it outputs patch embeddings.\n",
    "\n",
    "This output must then be flattened and projected linearly to derive the final forecasts, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"width: 600px; text-align: center;\">\n",
    "  <img src=\"img/time-LLM_06.png\" alt=\"Time-LLM\" width=\"200\">\n",
    "  <figcaption style=\"font-size: 12px;\">Fig: The output patch embeddings from the LLM are flattened and linearly projected to get the final predictions. Image by M. Jin, S. Wang, L. Ma, Z. Chu, J. Zhang, X. Shi, P. Chen, Y. Liang, Y. Li, S. Pan, Q. Wen from Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the flow through Time-LLM:\n",
    "\n",
    "- An input series is first patched and reprogrammed as a language task.\n",
    "- We append to it a prompt prefix specifying the context of the data, the instructions for the LLM, and input statistics.\n",
    "- The combined input is sent to the LLM.\n",
    "- The output embeddings are flattened and projected to generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the inner workings of Time-LLM, let's apply it in s small experiment using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with Time-LLM\n",
    "In this little experiment, we apply Time-LLM for time series forecasting and compare its performance to other models like N-HiTS and a simple multilayer perceptron (MLP).\n",
    "\n",
    "Before getting started, I must mention the following:\n",
    "\n",
    "I will be extending the library neuralforecast with Time-LLM, as I think that the original codebase of Time-LLM is hard to use.\n",
    "I am not the best prompt engineer and since Time-LLM relies on an LLM, you might be able to get better results than me with a better context prompt.\n",
    "The original paper reports performance obtained using LLaMA. Now, the license of LLaMA is restrictive, and it can only be used for research purposes. For that reason, I will use GPT-2. Of course, using other LLMs is likely going to give different results.\n",
    "I have limited time and compute power. I trained Time-LLM for 100 epochs on a single GPU. If you have more time and a better GPU, you can train the model for longer and potentially get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this experiment is available on GitHub.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "### Extend neuralforecast with Time-LLM\n",
    "For this experiment, I will implement Time-LLM in the library neuralforecast to make it easier and more flexible to use than the paper's implementation.\n",
    "\n",
    "Following the contribution guidelines of neuralforecast, we start by creating a `TimeLLM` class that inherits from `BaseWindows` which takes care of parsing batches of the input time series.\n",
    "\n",
    "Then, in the `__init__` function, we specify the parameters associated with Time-LLM, followed by the inherited parameters of `BaseWindows`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3606449070.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 23\u001b[1;36m\u001b[0m\n\u001b[1;33m    )\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "class TimeLLM(BaseWindows):\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 input_size,\n",
    "                 patch_len: int = 16,\n",
    "                 stride: int = 8,\n",
    "                 d_ff: int = 128,\n",
    "                 top_k: int = 5,\n",
    "                 d_llm: int = 768,\n",
    "                 d_model: int = 32,\n",
    "                 n_heads: int = 8,\n",
    "                 enc_in: int = 7,\n",
    "                 dec_in: int  = 7,\n",
    "                 llm = None,\n",
    "                 llm_config = None,\n",
    "                 llm_tokenizer = None,\n",
    "                 llm_num_hidden_layers = 32,\n",
    "                 llm_output_attention: bool = True,\n",
    "                 llm_output_hidden_states: bool = True,\n",
    "                 prompt_prefix: str = None,\n",
    "                 dropout: float = 0.1,\n",
    "                # Inherited parameters of BaseWindows\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block above, we see that we have parameters for patching, as well as for the LLM that we wish to use.\n",
    "\n",
    "Unlike the original implementation which uses LLaMA, this implementation allows the user to choose any LLM of their choice using the transformers library.\n",
    "\n",
    "Then, we reuse the same logic of the original implementation, but modify the forwardmethod to follow the guidelines of neuralforecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, windows_batch):\n",
    "    insample_y = windows_batch['insample_y']\n",
    "    \n",
    "    x = insample_y.unsqueeze(-1)\n",
    "    \n",
    "    y_pred = self.forecast(x)\n",
    "    y_pred = y_pred[:, -self.h:, :]\n",
    "    y_pred = self.loss.domain_map(y_pred)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block above, we use windows_batch to access an input batch and run it through Time-LLM. Then, we use self.loss.domain_map(y_pred) to map the output to the domain and shape of the chosen loss function. This is necessary for the model to actually train.\n",
    "\n",
    "For the fully detailed implementation, you can look here (it is still being actively developed, so changes might occur by the time you read this).\n",
    "\n",
    "Then, we simply add the model to the appropriate init file, and we can run pip install . to have access to the new model.\n",
    "\n",
    "Just like that, we can now use Time-LLM in neuralforecast.\n",
    "\n",
    "Predicting with Time-LLM\n",
    "We are now ready to forecasting with Time-LLM. Here, we use the simple air passenger's dataset.\n",
    "\n",
    "First, let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneuralforecast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeuralForecast\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import TimeLLM\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic, augment_calendar_df\n",
    "\n",
    "from transformers import GPT2Config, GPT2Model, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the dataset, which is conveniently included in neuralforecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'augment_calendar_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m AirPassengersPanel, calendar_cols \u001b[38;5;241m=\u001b[39m \u001b[43maugment_calendar_df\u001b[49m(df\u001b[38;5;241m=\u001b[39mAirPassengersPanel, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m Y_train_df \u001b[38;5;241m=\u001b[39m AirPassengersPanel[AirPassengersPanel\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m<\u001b[39mAirPassengersPanel[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m12\u001b[39m]]\n\u001b[0;32m      4\u001b[0m Y_test_df \u001b[38;5;241m=\u001b[39m AirPassengersPanel[AirPassengersPanel\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mAirPassengersPanel[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m12\u001b[39m]]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'augment_calendar_df' is not defined"
     ]
    }
   ],
   "source": [
    "AirPassengersPanel, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n",
    "\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]]\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to choose an LLM. For this experiment, let's go with GPT-2. We can load the model, its configuration and tokenizer from transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gpt2_config \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Config\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai-community/gpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m gpt2 \u001b[38;5;241m=\u001b[39m GPT2Model\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai-community/gpt2\u001b[39m\u001b[38;5;124m'\u001b[39m,config\u001b[38;5;241m=\u001b[39mgpt2_config)\n\u001b[0;32m      3\u001b[0m gpt2_tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai-community/gpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GPT2Config' is not defined"
     ]
    }
   ],
   "source": [
    "gpt2_config = GPT2Config.from_pretrained('openai-community/gpt2')\n",
    "gpt2 = GPT2Model.from_pretrained('openai-community/gpt2',config=gpt2_config)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('openai-community/gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define a prompt to explain the context of the data. Here, I used the following prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_prefix = \"The dataset contains data on monthly air passengers. There is a yearly seasonality\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can initialize the model using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TimeLLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m timellm \u001b[38;5;241m=\u001b[39m \u001b[43mTimeLLM\u001b[49m(h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m      2\u001b[0m                  input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m36\u001b[39m,\n\u001b[0;32m      3\u001b[0m                  llm\u001b[38;5;241m=\u001b[39mgpt2,\n\u001b[0;32m      4\u001b[0m                  llm_config\u001b[38;5;241m=\u001b[39mgpt2_config,\n\u001b[0;32m      5\u001b[0m                  llm_tokenizer\u001b[38;5;241m=\u001b[39mgpt2_tokenizer,\n\u001b[0;32m      6\u001b[0m                  prompt_prefix\u001b[38;5;241m=\u001b[39mprompt_prefix,\n\u001b[0;32m      7\u001b[0m                  max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      8\u001b[0m                  batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m,\n\u001b[0;32m      9\u001b[0m                  windows_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TimeLLM' is not defined"
     ]
    }
   ],
   "source": [
    "timellm = TimeLLM(h=12,\n",
    "                 input_size=36,\n",
    "                 llm=gpt2,\n",
    "                 llm_config=gpt2_config,\n",
    "                 llm_tokenizer=gpt2_tokenizer,\n",
    "                 prompt_prefix=prompt_prefix,\n",
    "                 max_steps=100,\n",
    "                 batch_size=24,\n",
    "                 windows_batch_size=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I am only training the reprogramming layer for 100 epochs and with a relatively small batch size. If you have more computing power, it might be better to increase max_steps, batch_size, and windows_batch_size.\n",
    "\n",
    "Then, we can train the model and get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = NeuralForecast(\n",
    "    models=[timellm],\n",
    "    freq='M'\n",
    ")\n",
    "\n",
    "nf.fit(df=Y_train_df, val_size=12)\n",
    "forecasts = nf.predict(futr_df=Y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"width: 600px; text-align: center;\">\n",
    "  <img src=\"img/time-LLM_07.png\" alt=\"Time-LLM\" width=\"600\">\n",
    "  <figcaption style=\"font-size: 12px;\">Fig: Visualizing the predictions of Time-LLM using GPT-2. Image by the author.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we can see that we successfully obtain time series forecasts while using the GPT-2 model, which I think is exciting and amazing!\n",
    "\n",
    "However, the predictions are not great. The peak is totally missed, and even the trend does not seem to be considered in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are many factors that could improve the performance, like:\n",
    "\n",
    "Training the model for longer. I trained for 100 epochs, but the paper uses 1000 epochs.\n",
    "Change LLM. I used GPT-2, but LLaMA, which was used in the paper, is much better. Maybe that using a Mistral model or Gemma would help.\n",
    "Better prompt. My prompt is very minimal, and perphas we can better engineer it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the goal is to show you how to use Time-LLM for your own use case, and not get state-of-the-art results. While my resources were restricted, we can still use any LLM we want to forecast any time series dataset, and that is the real advantage of this implementation.\n",
    "\n",
    "Still, for the sake of comleteness, let's compare Time-LLM to other models.\n",
    "\n",
    "Forecasting with N-BEATS and MLP\n",
    "My run with Time-LLM did not generate the best predictions, but let's still use other models to see how they perform.\n",
    "\n",
    "Specifically, we use N-BEATS and a simple MLP model. Here too, I will train for only 100 epochs, but feel free to train for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NBEATS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nbeats \u001b[38;5;241m=\u001b[39m \u001b[43mNBEATS\u001b[49m(h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m36\u001b[39m, max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      2\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLP(h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m36\u001b[39m, max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      4\u001b[0m nf \u001b[38;5;241m=\u001b[39m NeuralForecast(models\u001b[38;5;241m=\u001b[39m[nbeats, mlp], freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NBEATS' is not defined"
     ]
    }
   ],
   "source": [
    "nbeats = NBEATS(h=12, input_size=36, max_steps=100)\n",
    "mlp = MLP(h=12, input_size=36, max_steps=100)\n",
    "\n",
    "nf = NeuralForecast(models=[nbeats, mlp], freq='M')\n",
    "\n",
    "nf.fit(df=Y_train_df, val_size=12)\n",
    "forecasts = nf.predict(futr_df=Y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, N-BEATS and the MLP perform much better than Time-LLM, but again, keep in mind that my run of Time-LLM is far from being optimized.\n",
    "\n",
    "Still, let's evaluate the performance using the mean absolute error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.losses.numpy import mae\n",
    "\n",
    "mae_timellm = mae(Y_test_df['y'], Y_test_df['TimeLLM'])\n",
    "mae_nbeats = mae(Y_test_df['y'], Y_test_df['NBEATS'])\n",
    "mae_mlp = mae(Y_test_df['y'], Y_test_df['MLP'])\n",
    "\n",
    "data = {'Time-LLM': [mae_timellm], \n",
    "       'N-BEATS': [mae_nbeats],\n",
    "       'MLP': [mae_mlp]}\n",
    "\n",
    "metrics_df = pd.DataFrame(data=data)\n",
    "metrics_df.index = ['mae']\n",
    "\n",
    "metrics_df.style.highlight_min(color='lightgreen', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"width: 600px; text-align: center;\">\n",
    "  <img src=\"img/time-LLM_08.png\" alt=\"Time-LLM\" width=\"400\">\n",
    "  <figcaption style=\"font-size: 12px;\">Fig: MAE of all models on the air passenger's dataset. N-BEATS achieves the best performance. Image by the author.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, N-BEATS achieves the best performance as it has the lowest MAE.\n",
    "\n",
    "Again, the results for Time-LLM might be underwhelming, but there are many ways to improve upon my experiment since I had limited resources as aforementioned.\n",
    "\n",
    "Also, this was a very simple experiment on a toy dataset. The goal is to show how to use Time-LLM in any situation, rather than benchmark the model against other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My opinion on Time-LLM\n",
    "LLMs represent a giant leap forward in the field of NLP, and with their application in computer vision too, it only make sense to see them being applied for time series forecasting.\n",
    "\n",
    "Still, would I use Time-LLM in a forecasting project?\n",
    "\n",
    "Probably not.\n",
    "\n",
    "The reality is that Time-LLM requires a lot of computing power and memory. After all, we are working with an LLM.\n",
    "\n",
    "In fact, when reproducing the results from the paper using their script, training the model on a single dataset for 1000 epochs takes approximately 19 hours using a GPU!\n",
    "\n",
    "Plus, LLMs take a lot of memory space, with billions of parameter usually weighing a few gigabytes for the very large models. In comparison, we can train lightweight deep learning models in a few minutes and get very good forecasts.\n",
    "\n",
    "For those reasons, I think that the tradeoff between a possible increase in forecasts accuracy and the computing power and memory storage required to run such model is not worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, it is exciting to see that time series forecasting can now benefit from the advances in LLMs, which is an actively researched field. If LLMs get better and lighter, maybe Time-LLM will become an interesting option.\n",
    "\n",
    "Conclusion\n",
    "Time-LLM is a framework that allows any embedding-visible LLM to be used for time series forecasting.\n",
    "\n",
    "It first patches the input series to tokenize it and reprogram it as a language task, by traning a reprogramming layer.\n",
    "\n",
    "It also adds a prompt prefix to provide the LLM with the context of the dataset, the forecasting task, and some input statistics.\n",
    "\n",
    "Then, by keeping the LLM model intact, we can pass the reprogrammed patches and prompt to the LLM and obtain forecasts.\n",
    "\n",
    "As always, I think that each problem requires its unique solution. Make sure to test Time-LLM against other methods.\n",
    "\n",
    "Thanks for reading! I hope that you enjoyed it and that you learned something new!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
